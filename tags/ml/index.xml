<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ML on MlSense</title>
    <link>https://mlsense.github.io/tags/ml/</link>
    <description>Recent content in ML on MlSense</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 03 May 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://mlsense.github.io/tags/ml/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Time series and forecasting using R</title>
      <link>https://mlsense.github.io/posts/2017-05-03-time-series-forecasting/</link>
      <pubDate>Wed, 03 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/2017-05-03-time-series-forecasting/</guid>
      <description>Time series forecasting is a skill that few people claim to know. Machine learning is cool. And there are a lot of people interested in becoming a machine learning expert. But forecasting is something that is a little domain specific. Retailers like Walmart, Target use forecasting systems and tools to replenish their products in the stores. An excellent forecast system helps in winning the other pipelines of the supply chain.</description>
    </item>
    
    <item>
      <title>Diving into H2O with R</title>
      <link>https://mlsense.github.io/posts/h2o_with_r/</link>
      <pubDate>Tue, 28 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/h2o_with_r/</guid>
      <description>The problem Do you understand the pain when you have to train advanced machine learning algorithms like Random Forest on huge datasets? When there is a factor column that has way too many number of levels? When the time taken to train the model is so huge that you went to your pantry for snacks and came back, you are even done browsing 9gag but your model is still training, the code is still running?</description>
    </item>
    
    <item>
      <title>An illustrated introduction to adversarial validation - part 2</title>
      <link>https://mlsense.github.io/posts/introduction_to_adversarial_validation_part2/</link>
      <pubDate>Thu, 16 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/introduction_to_adversarial_validation_part2/</guid>
      <description>In the last post we talked about the idea of adversarial validation{:target=&amp;rdquo;_blank&amp;rdquo;} and how it helps the problem when your validation set result doesn&amp;rsquo;t comply with that of test set result. In this post, I will share the R code to help achieve the idea of adversarial validation. The data used would be from Numerai competition{:target=&amp;rdquo;_blank&amp;rdquo;}. Loading required packages library(randomForest) library(glmnet) library(data.table) library(MLmetrics) getwd() dir() Reading train and test data set train &amp;lt;- fread(&amp;#34;Data/numerai_training_data.</description>
    </item>
    
    <item>
      <title>An illustrated introduction to adversarial validation - part 1</title>
      <link>https://mlsense.github.io/posts/introduction_to_adversarial_validation_part1/</link>
      <pubDate>Wed, 15 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/introduction_to_adversarial_validation_part1/</guid>
      <description>You&amp;rsquo;d have heard about cross-validation - a common technique used in data-science process to avoid overfitting and many a times to tune the optimal parameters. Overfitting is when the model does well on training data but fails drastically on test data. The reason could be one of the following:
 The model is trying to map the exact findings of training data to test data instead of generalizing the patterns.</description>
    </item>
    
    <item>
      <title>How to use Git andÂ Github</title>
      <link>https://mlsense.github.io/posts/introduction_to_adversarial_validation/</link>
      <pubDate>Wed, 15 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/introduction_to_adversarial_validation/</guid>
      <description>I had taken this course - How to use git and github some time last year. This post is an amalgamation of the course notes and other tutorials I have completed in understanding git. I will talk about the most frequently used commands. If you already are confident of your git skills and wants more of practical tutorial, you should head to this post - git and github for data scientists.</description>
    </item>
    
    <item>
      <title>The curse of bias and variance</title>
      <link>https://mlsense.github.io/posts/the_curse_of_bias_and_variance/</link>
      <pubDate>Wed, 08 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/the_curse_of_bias_and_variance/</guid>
      <description>Statistics is the field of study where we try to draw conclusions about the population from a sample. Why do we talk about sample? Why can&amp;rsquo;t we get the conclusions about the population directly from the population? Let me illustrate this by an example. Let us say we want to understand which brand of beer do the people of Bangalore prefers? An interesting question. If I ask you this question, how would you approach this problem?</description>
    </item>
    
    <item>
      <title>Random Forest explained intuitively</title>
      <link>https://mlsense.github.io/posts/random_forest_explained_intuitively/</link>
      <pubDate>Tue, 18 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/random_forest_explained_intuitively/</guid>
      <description>A little fun fact. The image that you see in this post was drawn by Professor Adele Cutler&amp;rsquo;s son. Professor Adele is has worked very closely with Prof. Breiman on Random forest. Prof. Breiman wanted a simple photo that captured the essence of simplicity and comprehensiveness of Random forest algorithm. Let us get started. Random Forests algorithm has always fascinated me. I like how this algorithm can be easily explained to anyone without much hassle.</description>
    </item>
    
    <item>
      <title>ROC and AUC - The three lettered acronyms</title>
      <link>https://mlsense.github.io/posts/roc_and_auc_three_letter_acronym/</link>
      <pubDate>Mon, 26 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/roc_and_auc_three_letter_acronym/</guid>
      <description>Confession time I don&amp;rsquo;t feel bad to confess this that ROC curve, AUC, True-positive and related terms took quite some time for me to understand. If today I contemplate on the reasons why I found this topic confusing. The first would be there are not many resources that explains intuitively what these mean. They just jump to the terms and the mathematical formula for them. The second being I had not used them even in my project work.</description>
    </item>
    
  </channel>
</rss>