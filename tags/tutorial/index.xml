<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>tutorial on MlQuest</title>
    <link>https://mlsense.github.io/tags/tutorial/</link>
    <description>Recent content in tutorial on MlQuest</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Sep 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://mlsense.github.io/tags/tutorial/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Types of data in recommender systems</title>
      <link>https://mlsense.github.io/posts/types_of_data_recommender_system/</link>
      <pubDate>Thu, 27 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/types_of_data_recommender_system/</guid>
      <description>There are two ways in which we can collect data for building recommender systems — explicit and implicit. In this post, we will talk about both types of data, their characteristics and the challenges with them. Explicit feedback datasets The dictionary meaning of explicit is to state clearly and in detail. Explicit feedback data as the name suggests is an exact number given by a user to a product. Some of the examples of explicit feedback are ratings of movies by users on Netflix, ratings of products by users on Amazon.</description>
    </item>
    
    <item>
      <title>git and github for data scientists</title>
      <link>https://mlsense.github.io/posts/git_and_github_for_data_scientists/</link>
      <pubDate>Wed, 18 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/git_and_github_for_data_scientists/</guid>
      <description>Personal realisation It has been close to a year since I shifted to a start-up which incidentally got acquired after a month of my joining. Before this I used to work at WalmartLabs where we always wanted to use a version control system like git but it never took off properly. Now that I am working in this start-up I got to know that just taking a course on git/github doesn&amp;rsquo;t make you a master of this topic.</description>
    </item>
    
    <item>
      <title>Creating a virtual environment in Python</title>
      <link>https://mlsense.github.io/posts/virtual_environment_in_python/</link>
      <pubDate>Tue, 23 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/virtual_environment_in_python/</guid>
      <description>I was trying to get a virtual environment set up on Python 3 using mkvirtualenv but somehow the virtual environment was getting created on Python 2.7 (my system python). If you already know about virtual environments and why they are useful, you may skip the next two paragraphs. I came to know about virtual environments only recently. Virtual environment helps you create an isolated space wherein whatever packages you install won&amp;rsquo;t have an impact outside the environment.</description>
    </item>
    
    <item>
      <title>Common docker commands</title>
      <link>https://mlsense.github.io/posts/common_docker_commands/</link>
      <pubDate>Thu, 23 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/common_docker_commands/</guid>
      <description>I recently got to know about dockers. And I love it. For those who don&amp;rsquo;t know what dockers are. Here it is. Dockers help in software development in isolated frameworks.
Say, you are building an application named epsilon-X. epsilon-X relies on packages like numpy, scipy, pandas, other services, and software. Either you install each of these packages on your machine or you create an environment that has all these required packages along with the suitable versions.</description>
    </item>
    
    <item>
      <title>How to choose the probability cut-off in classification problem</title>
      <link>https://mlsense.github.io/posts/choosing_probability_cut_off_classification/</link>
      <pubDate>Thu, 18 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/choosing_probability_cut_off_classification/</guid>
      <description>Yesterday, I was taking a session on Data Science for few of my colleagues. The aim was to give a brief overview of machine learning. There were two of us taking the session. We had a rough idea what all we wanted to cover in the two hours session. I started the session starting with what machine learning is. The types of learning - supervised and unsupervised and the examples that fall into each of these.</description>
    </item>
    
    <item>
      <title>Tutorial on dplyr- a package for data manipulation in R</title>
      <link>https://mlsense.github.io/posts/tutorial_on_dplyr/</link>
      <pubDate>Mon, 15 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/tutorial_on_dplyr/</guid>
      <description>R is the most used tool in data science. It has no dearth of packages for specific use cases. There are three packages that I feel can get your most of the work done - ggplot2, dplyr, data.table.  ggplot2- Used for visualization. Also known as grammar of graphics. This package is used to plot graphs. The syntax is intuitive and easy to learn.
 dplyr- Used for data manipulation.</description>
    </item>
    
    <item>
      <title>The essence of machine learning is function estimation</title>
      <link>https://mlsense.github.io/posts/machine_learning_function_estimation/</link>
      <pubDate>Fri, 12 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/machine_learning_function_estimation/</guid>
      <description>Machine learning is cool. There is no denying in that. In this post we will try to make it a little uncool, well it will still be cool but you may start looking at it differently. Machine learning is not a black box. It is intuitive and this post is just to convey that. If I give you this function f(x) = x^2 + log(x) and ask to you tell me what will be f(2), you will first laugh at me and then run away to do something important.</description>
    </item>
    
    <item>
      <title>Time series and forecasting using R</title>
      <link>https://mlsense.github.io/posts/time_series_basics/</link>
      <pubDate>Wed, 03 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/time_series_basics/</guid>
      <description>Time series forecasting is a skill that few people claim to know. Machine learning is cool. And there are a lot of people interested in becoming a machine learning expert. But forecasting is something that is a little domain specific. Retailers like Walmart, Target use forecasting systems and tools to replenish their products in the stores. An excellent forecast system helps in winning the other pipelines of the supply chain.</description>
    </item>
    
    <item>
      <title>Diving into H2O with R</title>
      <link>https://mlsense.github.io/posts/h2o_with_r/</link>
      <pubDate>Tue, 28 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/h2o_with_r/</guid>
      <description>The problem Do you understand the pain when you have to train advanced machine learning algorithms like Random Forest on huge datasets? When there is a factor column that has way too many number of levels? When the time taken to train the model is so huge that you went to your pantry for snacks and came back, you are even done browsing 9gag but your model is still training, the code is still running?</description>
    </item>
    
    <item>
      <title>An illustrated introduction to adversarial validation - part 2</title>
      <link>https://mlsense.github.io/posts/introduction_to_adversarial_validation_part2/</link>
      <pubDate>Thu, 16 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/introduction_to_adversarial_validation_part2/</guid>
      <description>In the last post we talked about the idea of adversarial validation and how it helps the problem when your validation set result doesn&amp;rsquo;t comply with that of test set result. In this post, I will share the R code to help achieve the idea of adversarial validation. The data used would be from Numerai competition. Loading required packages library(randomForest) library(glmnet) library(data.table) library(MLmetrics) getwd() dir() Reading train and test data set train &amp;lt;- fread(&amp;#34;Data/numerai_training_data.</description>
    </item>
    
    <item>
      <title>An illustrated introduction to adversarial validation - part 1</title>
      <link>https://mlsense.github.io/posts/introduction_to_adversarial_validation_part1/</link>
      <pubDate>Wed, 15 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/introduction_to_adversarial_validation_part1/</guid>
      <description>You&amp;rsquo;d have heard about cross-validation - a common technique used in data-science process to avoid overfitting and many a times to tune the optimal parameters. Overfitting is when the model does well on training data but fails drastically on test data. The reason could be one of the following:
 The model is trying to map the exact findings of training data to test data instead of generalising the patterns.</description>
    </item>
    
    <item>
      <title>How to use Git and Github</title>
      <link>https://mlsense.github.io/posts/introduction_to_adversarial_validation/</link>
      <pubDate>Wed, 15 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/introduction_to_adversarial_validation/</guid>
      <description>I had taken this course - How to use git and github some time last year. This post is an amalgamation of the course notes and other tutorials I have completed in understanding git. I will talk about the most frequently used commands. If you already are confident of your git skills and wants more of practical tutorial, you should head to this post - git and github for data scientists.</description>
    </item>
    
    <item>
      <title>The curse of bias and variance</title>
      <link>https://mlsense.github.io/posts/the_curse_of_bias_and_variance/</link>
      <pubDate>Wed, 08 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/the_curse_of_bias_and_variance/</guid>
      <description>Statistics is the field of study where we try to draw conclusions about the population from a sample. Why do we talk about sample? Why can&amp;rsquo;t we get the conclusions about the population directly from the population? Let me illustrate this by an example. Let us say we want to understand which brand of beer do the people of Bangalore prefers? An interesting question. If I ask you this question, how would you approach this problem?</description>
    </item>
    
    <item>
      <title>Random Forest explained intuitively</title>
      <link>https://mlsense.github.io/posts/random_forest_explained_intuitively/</link>
      <pubDate>Tue, 18 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/random_forest_explained_intuitively/</guid>
      <description>A little fun fact. The image that you see in this post was drawn by Professor Adele Cutler&amp;rsquo;s son. Professor Adele is has worked very closely with Prof. Breiman on Random forest. Prof. Breiman wanted a simple photo that captured the essence of simplicity and comprehensiveness of Random forest algorithm. Let us get started. Random Forests algorithm has always fascinated me. I like how this algorithm can be easily explained to anyone without much hassle.</description>
    </item>
    
    <item>
      <title>Improve runtime of Random Forest in R</title>
      <link>https://mlsense.github.io/posts/improve_runtime_random_forest_r/</link>
      <pubDate>Thu, 13 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/improve_runtime_random_forest_r/</guid>
      <description>There are two ways one can write the code to train a random forest model in R. Both the ways are listed below. A normal and frequent way of writing the command to train the random forest model is something like this. rfModel &amp;lt;- randomForest(Survived~. , data = trainSample[, -c(6, 8, 9)])  Notice the ~ sign. We call this the formula way of writing. Another way of writing the command to train the random forest model is shown below.</description>
    </item>
    
    <item>
      <title>How to install a package of a particular version in R</title>
      <link>https://mlsense.github.io/posts/install_a_package_particular_version_in_r/</link>
      <pubDate>Wed, 05 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/install_a_package_particular_version_in_r/</guid>
      <description>I recently tried installing caret package in R using install.packages(&amp;lsquo;caret&amp;rsquo;, dependencies=T) 
Normally this installation of package works and I continue to work with the functions associated with the package. When I tried including the package using library(caret) 
I got the following error.
Error in loadNamespace(j  R was not able to install this dependency package- pbkrtest. So I tried installing it separately, again using install.package(&amp;lsquo;pbkrtest&amp;rsquo;, dependencies=T)</description>
    </item>
    
    <item>
      <title>Shell commands come in handy for a data scientist</title>
      <link>https://mlsense.github.io/posts/shell_commands_for_data_scientist/</link>
      <pubDate>Fri, 30 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/shell_commands_for_data_scientist/</guid>
      <description>I am no expert of shell commands. I have been using them for quite some time and thought I give an attempt to list down the most common commands. I am writing these mostly from the perspective of a data-science guy. Let us get started. I will use the file- ‘data.txt’ to illustrate these commands. ‘data.txt’ is a file having 200 rows and 8 columns. You can access the data here.</description>
    </item>
    
    <item>
      <title>ROC and AUC - The three lettered acronyms</title>
      <link>https://mlsense.github.io/posts/roc_and_auc_three_letter_acronym/</link>
      <pubDate>Mon, 26 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/roc_and_auc_three_letter_acronym/</guid>
      <description>Confession time I don&amp;rsquo;t feel bad to confess this that ROC curve, AUC, True-positive and related terms took quite some time for me to understand. If today I contemplate on the reasons why I found this topic confusing. The first would be there are not many resources that explains intuitively what these mean. They just jump to the terms and the mathematical formula for them. The second being I had not used them even in my project work.</description>
    </item>
    
    <item>
      <title>Hadoop Streaming</title>
      <link>https://mlsense.github.io/posts/hadoop_streaming/</link>
      <pubDate>Mon, 29 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/hadoop_streaming/</guid>
      <description>A few days ago, I had written a post on The Big Data Problem which attempted to understand why we need big data and what the fuss is all about. You may want to read it here. Having understood why we need big data, let’s understand how we can go about analyzing the same. What is the way out to do analysis on big data? The solution is Streaming…Hadoop Streaming.</description>
    </item>
    
    <item>
      <title>The Big Data Problem</title>
      <link>https://mlsense.github.io/posts/big_data_problem/</link>
      <pubDate>Wed, 29 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/big_data_problem/</guid>
      <description>Big data has become a sensation these days. Anyone and everyone wants to use this in their discussions. When I was still in my college and preparing for campus placements, I had attended almost all the pre-placement talks that companies gave to its prospective candidates. American Express was one such company that had talked extensively about big data and hadoop in their presentation. I remember clearly, the blank faces that all of us had.</description>
    </item>
    
    <item>
      <title>Vim/Vi editor shortcuts</title>
      <link>https://mlsense.github.io/posts/vim_shortcuts/</link>
      <pubDate>Mon, 22 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>https://mlsense.github.io/posts/vim_shortcuts/</guid>
      <description>Repetitive tasks should be done using as many shortcuts as possible. You are not doing anything new and hence not even an extra minute should be spent on doing the same. This post refers to the shortcuts that come in handy when working on the vi/vim editor.
This is not an exhaustive list. These are the ones I use frequently. Feel free to comment down your favorite shortcuts. Navigation keys  0 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; Moves cursor to the start of the line $ &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; Moves cursor to the end of the line w &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; Moves forward one word b &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; Moves backward one word</description>
    </item>
    
  </channel>
</rss>